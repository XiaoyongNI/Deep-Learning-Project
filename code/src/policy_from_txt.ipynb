{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "policy_txt = '/Users/wangrui/Projects/Deep-Learning-Project_RL/Complex-YOLOv4/dataset/kitti/resnet50_policies_1.txt'\n",
    "hr_txt = '/Users/wangrui/Projects/Deep-Learning-Project_RL/Complex-YOLOv4/dataset/kitti/fine_detector_boxid.txt'\n",
    "lr_txt = '/Users/wangrui/Projects/Deep-Learning-Project_RL/Complex-YOLOv4/dataset/kitti/coarse_detector_boxid.txt'\n",
    "id_txt ='/Users/wangrui/Projects/Deep-Learning-Project_RL/Complex-YOLOv4/dataset/kitti/val_id_16.txt'\n",
    "gt_txt = '/Users/wangrui/Projects/Deep-Learning-Project_RL/Complex-YOLOv4/dataset/kitti/ground_truth_16.txt'\n",
    "policy_mat = np.loadtxt(policy_txt)\n",
    "hr_mat = np.loadtxt(hr_txt)\n",
    "lr_mat = np.loadtxt(lr_txt)\n",
    "\n",
    "id_mat = np.loadtxt(id_txt)\n",
    "gt_mat = np.loadtxt(gt_txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (list).\n",
    "        conf:  Objectness value from 0-1 (list).\n",
    "        pred_cls: Predicted object classes (list).\n",
    "        target_cls: True object classes (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    ap, p, r = [], [], []\n",
    "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 and n_gt == 0:\n",
    "            continue\n",
    "        elif n_p == 0 or n_gt == 0:\n",
    "            ap.append(0)\n",
    "            r.append(0)\n",
    "            p.append(0)\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum()\n",
    "            tpc = (tp[i]).cumsum()\n",
    "\n",
    "            # Recall\n",
    "            recall_curve = tpc / (n_gt + 1e-16)\n",
    "            r.append(recall_curve[-1])\n",
    "\n",
    "            # Precision\n",
    "            precision_curve = tpc / (tpc + fpc)\n",
    "            p.append(precision_curve[-1])\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            ap.append(compute_ap(recall_curve, precision_curve))\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangrui/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "Computing AP: 100%|██████████| 3/3 [00:00<00:00, 529.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.8301105  0.47881356 0.55      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def find_idx(this_po)\n",
    "###### Ensemble RL\n",
    "eval_len,_ = np.shape(policy_mat)\n",
    "true_positives, pred_scores, pred_labels, labels = [],[],[],[]\n",
    "gt_labels = []\n",
    "for i in range(eval_len):\n",
    "    this_po = policy_mat[i,:]\n",
    "    this_index = np.where(np.logical_and(id_mat[:,0] == this_po[0], id_mat[:,1] == this_po[1]))\n",
    "    this_index = this_index[0]\n",
    "    if this_index.size == 0:\n",
    "        continue\n",
    "    gt_index = np.where(gt_mat[:,0]==this_index)[0]\n",
    "    if len(gt_index)==1:\n",
    "        gt_labels.append(gt_mat[gt_index,2])\n",
    "    else:\n",
    "        gt_labels+= list(gt_mat[gt_index,2])\n",
    "        \n",
    "    \n",
    "    if this_po[2]>0:\n",
    "        hr_index = np.where(hr_mat[:,0]==this_index)[0]\n",
    "        \n",
    "        if len(hr_index) == 1:\n",
    "            true_positives.append(np.double(hr_mat[hr_index,2]>=0.5))\n",
    "        else:\n",
    "            true_positives += list(np.double(hr_mat[hr_index,2]>=0.5))\n",
    "        gt_box_id = hr_mat[hr_index,5]\n",
    "        labels += list(gt_mat[gt_index[gt_box_id.astype('int')],2])\n",
    "        pred_scores += list(hr_mat[hr_index,3])\n",
    "        pred_labels += list(hr_mat[hr_index,4])\n",
    "\n",
    "    else:\n",
    "        lr_index = np.where(lr_mat[:,0]==this_index)[0]\n",
    "        if len(lr_index) == 1:\n",
    "            true_positives.append(np.double(lr_mat[lr_index,2]>=0.5))\n",
    "        else:\n",
    "            true_positives += list(np.double(lr_mat[lr_index,2]>=0.5))\n",
    "        gt_box_id = lr_mat[lr_index,5]\n",
    "        labels += list(gt_mat[gt_index[gt_box_id.astype('int')],2])\n",
    "        pred_scores += list(lr_mat[lr_index,3])\n",
    "        pred_labels += list(lr_mat[lr_index,4])\n",
    "        \n",
    "\n",
    "#         sample_metrics += \n",
    "        \n",
    "precision, recall, AP, f1, ap_class = ap_per_class(np.array(true_positives),np.array(pred_scores), np.array(pred_labels), np.array(gt_labels))\n",
    "print('precision',precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>>\t Class  (0): precision = 0.8209, recall = 0.8945, AP = 0.8529, f1: 0.8561\n",
      "\t>>>\t Class  (1): precision = 0.5409, recall = 0.7000, AP = 0.4607, f1: 0.6103\n",
      "\t>>>\t Class  (2): precision = 0.6528, recall = 0.7705, AP = 0.6253, f1: 0.7068\n"
     ]
    }
   ],
   "source": [
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangrui/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "Computing AP: 100%|██████████| 3/3 [00:00<00:00, 498.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.78160159 0.29803922 0.3047619 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def find_idx(this_po)\n",
    "###### Ensemble RL\n",
    "eval_len,_ = np.shape(policy_mat)\n",
    "true_positives, pred_scores, pred_labels, labels = [],[],[],[]\n",
    "gt_labels = []\n",
    "for i in range(eval_len):\n",
    "    this_po = policy_mat[i,:]\n",
    "    this_index = np.where(np.logical_and(id_mat[:,0] == this_po[0], id_mat[:,1] == this_po[1]))\n",
    "    this_index = this_index[0]\n",
    "    if this_index.size == 0:\n",
    "        continue\n",
    "    gt_index = np.where(gt_mat[:,0]==this_index)[0]\n",
    "    if len(gt_index)==1:\n",
    "        gt_labels.append(gt_mat[gt_index,2])\n",
    "    else:\n",
    "        gt_labels+= list(gt_mat[gt_index,2])\n",
    "        \n",
    "    \n",
    "    if np.random.rand()<0.2601:\n",
    "        hr_index = np.where(hr_mat[:,0]==this_index)[0]\n",
    "        \n",
    "        if len(hr_index) == 1:\n",
    "            true_positives.append(np.double(hr_mat[hr_index,2]>0.5))\n",
    "        else:\n",
    "            true_positives += list(np.double(hr_mat[hr_index,2]>0.5))\n",
    "        gt_box_id = hr_mat[hr_index,5]\n",
    "        labels += list(gt_mat[gt_index[gt_box_id.astype('int')],2])\n",
    "        pred_scores += list(hr_mat[hr_index,3])\n",
    "        pred_labels += list(hr_mat[hr_index,4])\n",
    "\n",
    "    else:\n",
    "        lr_index = np.where(lr_mat[:,0]==this_index)[0]\n",
    "        if len(lr_index) == 1:\n",
    "            true_positives.append(np.double(lr_mat[lr_index,2]>0.5))\n",
    "        else:\n",
    "            true_positives += list(np.double(lr_mat[lr_index,2]>0.5))\n",
    "        gt_box_id = lr_mat[lr_index,5]\n",
    "        labels += list(gt_mat[gt_index[gt_box_id.astype('int')],2])\n",
    "        pred_scores += list(lr_mat[lr_index,3])\n",
    "        pred_labels += list(lr_mat[lr_index,4])\n",
    "        \n",
    "\n",
    "#         sample_metrics += \n",
    "        \n",
    "precision, recall, AP, f1, ap_class = ap_per_class(np.array(true_positives),np.array(pred_scores), np.array(pred_labels), np.array(gt_labels))\n",
    "print('precision',precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble\n",
      "\t>>>\t Class  (0): precision = 0.8392, recall = 0.9058, AP = 0.8719, f1: 0.8712\n",
      "\t>>>\t Class  (1): precision = 0.5952, recall = 0.7353, AP = 0.5301, f1: 0.6579\n",
      "\t>>>\t Class  (2): precision = 0.7353, recall = 0.8197, AP = 0.7194, f1: 0.7752\n"
     ]
    }
   ],
   "source": [
    "print('Ensemble')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL random\n",
      "\t>>>\t Class  (0): precision = 0.7816, recall = 0.8833, AP = 0.8065, f1: 0.8294\n",
      "\t>>>\t Class  (1): precision = 0.2980, recall = 0.4471, AP = 0.1812, f1: 0.3576\n",
      "\t>>>\t Class  (2): precision = 0.3048, recall = 0.5246, AP = 0.2085, f1: 0.3855\n"
     ]
    }
   ],
   "source": [
    "print('ALL random')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg1.6G\n",
      "\t>>>\t Class  (0): precision = 0.8139, recall = 0.8863, AP = 0.8332, f1: 0.8485\n",
      "\t>>>\t Class  (1): precision = 0.5434, recall = 0.7000, AP = 0.4589, f1: 0.6118\n",
      "\t>>>\t Class  (2): precision = 0.6133, recall = 0.7541, AP = 0.5868, f1: 0.6765\n"
     ]
    }
   ],
   "source": [
    "print('Reg1.6G')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet\n",
      "\t>>>\t Class  (0): precision = 0.8301, recall = 0.8990, AP = 0.8604, f1: 0.8632\n",
      "\t>>>\t Class  (1): precision = 0.4788, recall = 0.6647, AP = 0.3946, f1: 0.5567\n",
      "\t>>>\t Class  (2): precision = 0.5500, recall = 0.7213, AP = 0.5536, f1: 0.6241\n"
     ]
    }
   ],
   "source": [
    "print('Resnet')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL low\n",
      "\t>>>\t Class  (0): precision = 0.7567, recall = 0.8654, AP = 0.7713, f1: 0.8074\n",
      "\t>>>\t Class  (1): precision = 0.1929, recall = 0.3176, AP = 0.0957, f1: 0.2400\n",
      "\t>>>\t Class  (2): precision = 0.2692, recall = 0.4590, AP = 0.1702, f1: 0.3394\n"
     ]
    }
   ],
   "source": [
    "print('ALL low')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL high\n",
      "\t>>>\t Class  (0): precision = 0.8650, recall = 0.9342, AP = 0.9190, f1: 0.8982\n",
      "\t>>>\t Class  (1): precision = 0.6567, recall = 0.7765, AP = 0.6292, f1: 0.7116\n",
      "\t>>>\t Class  (2): precision = 0.8358, recall = 0.9180, AP = 0.8872, f1: 0.8750\n"
     ]
    }
   ],
   "source": [
    "print('ALL high')\n",
    "for idx, cls in enumerate(ap_class):\n",
    "    print(\"\\t>>>\\t Class  ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, precision[idx], recall[idx], AP[idx], f1[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
